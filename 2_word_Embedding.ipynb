{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOU4Qp8USnWVTpyNXJevd5A",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AIWalaBro/Complete_NLP/blob/main/2_word_Embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word Embeddings\n"
      ],
      "metadata": {
        "id": "FcYCPdRLklIX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Whenever we apply any algorithm to textual data, we need to convert the text to a numeric form. Hence, there arises a need for some pre-processing techniques that can convert our text to numbers.\n",
        "\n",
        "This is where the concepts of One-Hot Encoding(OHE), Bag-of-Words (BoW) and TF-IDF come into play. Both BoW and TF-IDF are techniques that help us convert text sentences into numeric vectors.\n",
        "\n",
        "### Example:\n",
        "- Review 1: This movie is very scary and long\n",
        "- Review 2: This movie is not scary and is slow\n",
        "- Review 3: This movie is spooky and good\n",
        "\n",
        "You can see that there are some contrasting reviews about the movie as well as the length and pace of the movie. Imagine looking at a thousand reviews like these. Clearly, there is a lot of interesting insights we can draw from them and build upon them to gauge how well the movie performed.\n",
        "\n",
        "However, as we saw above, we cannot simply give these sentences to a machine learning model and ask it to tell us whether a review was positive or negative. We need to perform certain text preprocessing steps.\n",
        "\n",
        "\n",
        "**Word Embedding is one such technique where we can represent the text using vectors. The more popular forms of word embeddings are:**\n",
        "\n",
        "- 1.BoW, which stands for Bag of Words\n",
        "- 2.TF-IDF, which stands for Term Frequency-Inverse Document Frequency"
      ],
      "metadata": {
        "id": "jo-59JjJMD6t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Why do we need Word Embeddings?\n",
        "\n",
        "As we know many Machine Learning algorithms and almost all Deep Learning Architectures are not capable of processing strings or plain text in their raw form.\n",
        "\n",
        "They require numerical numbers as inputs to perform any sort of task, such as classification, regression, clustering, etc. Also, from the huge amount of data that is present in the text format, it is imperative to extract some knowledge out of it and build any useful applications.\n",
        "\n",
        "In short, we can say that to build any model in machine learning or deep learning, the final level data has to be in numerical form because models don’t understand text or image data directly as humans do.\n",
        "\n",
        "### How did NLP models learn patterns from text data?\n",
        "\n",
        "To convert the text data into numerical data, we need some smart ways which are known as vectorization, or in the NLP world, it is known as Word embeddings.\n",
        "\n",
        "Vectorization or word embedding is the process of converting text data to numerical vectors. Later those vectors are used to build various machine learning models. In this manner, we say this as extracting features with the help of text with an aim to build multiple natural languages, processing models, etc."
      ],
      "metadata": {
        "id": "viJOLGyUMYfi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Familiar with Terminologies\n",
        "\n",
        "Before understanding Vectorization, below are the few terms that you need to understand.\n",
        "\n",
        "**Document**\n",
        "A document is a single text data point. For Example, a review of a particular product by the user.\n",
        "\n",
        "**Corpus**\n",
        "It a collection of all the documents present in our dataset.\n",
        "\n",
        "**Feature**\n",
        "Every unique word in the corpus is considered as a feature.\n",
        "\n",
        "**For Example**, Let’s consider the 2 documents shown below:\n",
        "\n",
        "**Sentences:**\n",
        "**Dog hates a cat. It loves to go out and play.**\n",
        "**Cat loves to play with a ball.**\n",
        "\n",
        "We can build a corpus from the above 2 documents just by combining them.\n",
        "\n",
        "**Corpus = “Dog hates a cat. It loves to go out and play. Cat loves to play with a ball.”**\n",
        "\n",
        "And features will be all unique words:\n",
        "\n",
        "**Fetaures: [‘and’, ‘ball’, ‘cat’, ‘dog’, ‘go’, ‘hates’, ‘it’, ‘loves’, ‘out’, ‘play’, ‘to’, ‘with’]**"
      ],
      "metadata": {
        "id": "nGzzx2wcMgsz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## One-Hot Encoding (OHE)\n",
        "\n",
        "In this technique, we represent each unique word in vocabulary by setting a unique token with value 1 and rest 0 at other positions in the vector. In simple words, a vector representation of a one-hot encoded vector represents in the form of 1, and 0 where 1 stands for the position where the word exists and 0 everywhere else.\n",
        "\n",
        "![2.png](attachment:2.png)\n",
        "\n",
        "Example:\n",
        "\n",
        "**Sentence: I am teaching NLP in Python**\n",
        "\n",
        "A word in this sentence may be “NLP”, “Python”, “teaching”, etc.\n",
        "\n",
        "Since a dictionary is defined as the list of all unique words present in the sentence. So, a dictionary may look like –\n",
        "\n",
        "**Dictionary: [‘I’, ’am’, ’teaching’,’ NLP’,’ in’, ’Python’] **\n",
        "\n",
        "Therefore, the vector representation in this format according to the above dictionary is\n",
        "\n",
        "**Vector for NLP: [0,0,0,1,0,0]**\n",
        "\n",
        "**Vector for Python:  [0,0,0,0,0,1]**\n",
        "\n",
        "This is just a very simple method to represent a word in vector form."
      ],
      "metadata": {
        "id": "1quBc2r-MoIB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Disadvantages of One-hot Encoding\n",
        "\n",
        "1. One of the disadvantages of One-hot encoding is that the Size of the vector is equal to the count of unique words in the vocabulary.\n",
        "\n",
        "2. One-hot encoding does not capture the relationships between different words. Therefore, it does not convey information about the context."
      ],
      "metadata": {
        "id": "upP6g6HHMtJu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bag of Words (BoW) Model\n",
        "\n",
        "The Bag of Words (BoW) model is the simplest form of text representation in numbers. Like the term itself, we can represent a sentence as a bag of words vector (a string of numbers).\n",
        "\n",
        "Let’s recall the three types of movie reviews we saw earlier:\n",
        "\n",
        "- This movie is very scary and long\n",
        "- This movie is not scary and is slow\n",
        "- This movie is spooky and good\n",
        "\n",
        "We will first build a vocabulary from all the unique words in the above three reviews. The vocabulary consists of these 11 words: ‘This’, ‘movie’, ‘is’, ‘very’, ‘scary’, ‘and’, ‘long’, ‘not’,  ‘slow’, ‘spooky’,  ‘good’.\n",
        "\n",
        "We can now take each of these words and mark their occurrence in the three movie reviews above with 1s and 0s. This will give us 3 vectors for 3 reviews:\n",
        "\n",
        "![1.jpg](attachment:1.jpg)\n",
        "\n",
        "Vector of Review 1: [1 1 1 1 1 1 1 0 0 0 0]\n",
        "\n",
        "Vector of Review 2: [1 1 2 0 0 1 1 0 1 0 0]\n",
        "\n",
        "Vector of Review 3: [1 1 1 0 0 0 1 0 0 1 1]\n",
        "\n",
        "And that’s the core idea behind a Bag of Words (BoW) model."
      ],
      "metadata": {
        "id": "dTCDsGLdMwyc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Drawbacks of using a Bag-of-Words (BoW) Model\n",
        "\n",
        "In the above example, we can have vectors of length 11. However, we start facing issues when we come across new sentences:\n",
        "\n",
        "1. If the new sentences contain new words, then our vocabulary size would increase and thereby, the length of the vectors would increase too.\n",
        "\n",
        "\n",
        "2. Additionally, the vectors would also contain many 0s, thereby resulting in a sparse matrix (which is what we would like to avoid)\n",
        "\n",
        "\n",
        "3. We are retaining no information on the grammar of the sentences nor on the ordering of the words in the text."
      ],
      "metadata": {
        "id": "KKDuFFBrM0O9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The bag-of-words model converts text into fixed-length vectors by counting how many times each word appears.\n",
        "\n",
        "- Text processing is necessary.\n",
        "\n",
        "- Text processing is necessary and important.\n",
        "\n",
        "- Text processing is easy.\n",
        "\n",
        "\n",
        "If we take out the unique words in all these sentences, the vocabulary will consist of these 7 words: {‘Text’, ’processing’, ’is’, ’necessary’, ’and’, ’important, ’easy’}.\n",
        "\n",
        "To carry out bag-of-words, we will simply have to count the number of times each word appears in each of the documents.\n",
        "\n",
        "![2.jpg](attachment:2.jpg)\n",
        "\n",
        "- we have the following vectors for each of the documents of fixed length -7\n",
        "\n",
        "Document 1: [1,1,1,1,0,0,0]\n",
        "\n",
        "Document 2: [1,1,1,1,1,1,0]\n",
        "\n",
        "Document 3: [1,1,1,0,0,0,1]\n",
        "\n",
        "#### Limitations of Bag-of-Words\n",
        "\n",
        "If we deploy bag-of-words to generate vectors for large documents, the vectors would be of large sizes and would also have too many null values leading to the creation of sparse vectors.\n",
        "\n",
        "Bag-of-words does not bring in any information on the meaning of the text.\n",
        "\n",
        "*For example:* if we consider these two sentences – “Text processing is easy but tedious.” and “Text processing is tedious but easy.” – a bag-of-words model would create the same vectors for both of them, even though they have different meanings."
      ],
      "metadata": {
        "id": "M6xbsdvUM4fz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example:\n",
        "    \n",
        "- This burger is very tasty and affordable.\n",
        "- This burger is not tasty and is affordable.\n",
        "- This burger is very very delicious.\n",
        "\n",
        "**Unique words: [“and”, “affordable.”, “delicious.”,  “is”, “not”, “burger”, “tasty”, “this”, “very”]**"
      ],
      "metadata": {
        "id": "K6rlf7tZNBVE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ieTvOdQNi3eU",
        "outputId": "7b5046f4-0f0a-4430-d76e-fe803edeaa35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1 1 1 0 1 0 1 1 1]\n",
            " [1 1 1 0 2 1 1 1 0]\n",
            " [0 0 1 1 1 0 0 1 2]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "corpus = [\"This burger is very tasty and affordable.\", \"This burger is not tasty and is affordable.\", \"This burger is very very delicious.\"]\n",
        "countvect = CountVectorizer()\n",
        "x = countvect.fit_transform(corpus)\n",
        "result = x.toarray()\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "# execute the text here as :\n",
        "text = \"\"\"If learning what is data science sounded interesting, understanding what does this job roles is all about\n",
        "                will me much more interesting to you.\n",
        "                Data scientists are among the most recent analytical data professionals who have the technical\n",
        "                ability to handle complicated issues as well as the desire to investigate what questions need to be answered.\n",
        "                They're a mix of mathematicians, computer scientists, and trend forecasters.\n",
        "                They're also in high demand and well-paid because they work in both the business and IT sectors.\n",
        "                On a daily basis, a data scientist may do the following tasks.\"\"\""
      ],
      "metadata": {
        "id": "LRCScGvHk_4j"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7yL94j12pEPg",
        "outputId": "ffb89bf9-1428-4e74-a526-b5b824ef6d50"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "dataset = nltk.sent_tokenize(text)\n",
        "for i in range(len(dataset)):\n",
        "  dataset[i] = dataset[i].lower()\n",
        "  dataset[i]= re.sub(r'\\W',' ',dataset[i])\n",
        "  dataset[i] = re.sub(r'\\s+',' ',dataset[i])"
      ],
      "metadata": {
        "id": "6HoGZubKnqL_"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5A_dYXR_o5ww",
        "outputId": "0426dedb-d032-4554-ea2e-2d05645f0a70"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['if learning what is data science sounded interesting understanding what does this job roles is all about will me much more interesting to you ',\n",
              " 'data scientists are among the most recent analytical data professionals who have the technical ability to handle complicated issues as well as the desire to investigate what questions need to be answered ',\n",
              " 'they re a mix of mathematicians computer scientists and trend forecasters ',\n",
              " 'they re also in high demand and well paid because they work in both the business and it sectors ',\n",
              " 'on a daily basis a data scientist may do the following tasks ']"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating word tokenization from scratch"
      ],
      "metadata": {
        "id": "mMPBqQ71quu1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word2count = {}\n",
        "\n",
        "for data in dataset:\n",
        "  words = nltk.word_tokenize(data)\n",
        "  for word in words:\n",
        "    if word not in word2count.keys():\n",
        "      word2count[word] = 1\n",
        "    else:\n",
        "      word2count[word] += 1"
      ],
      "metadata": {
        "id": "9FUVRK2_pmTN"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word2count"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JrKEJ4jorKoR",
        "outputId": "90a27d7b-8563-49d3-a927-447c1925e043"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'if': 1,\n",
              " 'learning': 1,\n",
              " 'what': 3,\n",
              " 'is': 2,\n",
              " 'data': 4,\n",
              " 'science': 1,\n",
              " 'sounded': 1,\n",
              " 'interesting': 2,\n",
              " 'understanding': 1,\n",
              " 'does': 1,\n",
              " 'this': 1,\n",
              " 'job': 1,\n",
              " 'roles': 1,\n",
              " 'all': 1,\n",
              " 'about': 1,\n",
              " 'will': 1,\n",
              " 'me': 1,\n",
              " 'much': 1,\n",
              " 'more': 1,\n",
              " 'to': 4,\n",
              " 'you': 1,\n",
              " 'scientists': 2,\n",
              " 'are': 1,\n",
              " 'among': 1,\n",
              " 'the': 5,\n",
              " 'most': 1,\n",
              " 'recent': 1,\n",
              " 'analytical': 1,\n",
              " 'professionals': 1,\n",
              " 'who': 1,\n",
              " 'have': 1,\n",
              " 'technical': 1,\n",
              " 'ability': 1,\n",
              " 'handle': 1,\n",
              " 'complicated': 1,\n",
              " 'issues': 1,\n",
              " 'as': 2,\n",
              " 'well': 2,\n",
              " 'desire': 1,\n",
              " 'investigate': 1,\n",
              " 'questions': 1,\n",
              " 'need': 1,\n",
              " 'be': 1,\n",
              " 'answered': 1,\n",
              " 'they': 3,\n",
              " 're': 2,\n",
              " 'a': 3,\n",
              " 'mix': 1,\n",
              " 'of': 1,\n",
              " 'mathematicians': 1,\n",
              " 'computer': 1,\n",
              " 'and': 3,\n",
              " 'trend': 1,\n",
              " 'forecasters': 1,\n",
              " 'also': 1,\n",
              " 'in': 2,\n",
              " 'high': 1,\n",
              " 'demand': 1,\n",
              " 'paid': 1,\n",
              " 'because': 1,\n",
              " 'work': 1,\n",
              " 'both': 1,\n",
              " 'business': 1,\n",
              " 'it': 1,\n",
              " 'sectors': 1,\n",
              " 'on': 1,\n",
              " 'daily': 1,\n",
              " 'basis': 1,\n",
              " 'scientist': 1,\n",
              " 'may': 1,\n",
              " 'do': 1,\n",
              " 'following': 1,\n",
              " 'tasks': 1}"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import heapq\n",
        "freq_words = heapq.nlargest(100, word2count,key= word2count.get)"
      ],
      "metadata": {
        "id": "XU6_8SS8sDG_"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "freq_words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMEr94VmuBze",
        "outputId": "7f5321d4-ca4f-457a-cebc-39eefc20b715"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['the',\n",
              " 'data',\n",
              " 'to',\n",
              " 'what',\n",
              " 'they',\n",
              " 'a',\n",
              " 'and',\n",
              " 'is',\n",
              " 'interesting',\n",
              " 'scientists',\n",
              " 'as',\n",
              " 'well',\n",
              " 're',\n",
              " 'in',\n",
              " 'if',\n",
              " 'learning',\n",
              " 'science',\n",
              " 'sounded',\n",
              " 'understanding',\n",
              " 'does',\n",
              " 'this',\n",
              " 'job',\n",
              " 'roles',\n",
              " 'all',\n",
              " 'about',\n",
              " 'will',\n",
              " 'me',\n",
              " 'much',\n",
              " 'more',\n",
              " 'you',\n",
              " 'are',\n",
              " 'among',\n",
              " 'most',\n",
              " 'recent',\n",
              " 'analytical',\n",
              " 'professionals',\n",
              " 'who',\n",
              " 'have',\n",
              " 'technical',\n",
              " 'ability',\n",
              " 'handle',\n",
              " 'complicated',\n",
              " 'issues',\n",
              " 'desire',\n",
              " 'investigate',\n",
              " 'questions',\n",
              " 'need',\n",
              " 'be',\n",
              " 'answered',\n",
              " 'mix',\n",
              " 'of',\n",
              " 'mathematicians',\n",
              " 'computer',\n",
              " 'trend',\n",
              " 'forecasters',\n",
              " 'also',\n",
              " 'high',\n",
              " 'demand',\n",
              " 'paid',\n",
              " 'because',\n",
              " 'work',\n",
              " 'both',\n",
              " 'business',\n",
              " 'it',\n",
              " 'sectors',\n",
              " 'on',\n",
              " 'daily',\n",
              " 'basis',\n",
              " 'scientist',\n",
              " 'may',\n",
              " 'do',\n",
              " 'following',\n",
              " 'tasks']"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = []\n",
        "for data in dataset:\n",
        "  vector = []\n",
        "  for word in freq_words:\n",
        "    if word in nltk.word_tokenize(data):\n",
        "      vector.append(1)\n",
        "    else:\n",
        "      vector.append(0)\n",
        "  X.append(vector)\n",
        "X = np.asarray(X)\n",
        "print(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nRxADI4XuFzG",
        "outputId": "9a74bb49-4a4c-4a4f-9bb5-b6fed7cd14e1"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 1 1 1 0 0 0 1 1 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0]\n",
            " [1 1 1 1 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1\n",
            "  1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0]\n",
            " [0 0 0 0 1 1 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0]\n",
            " [1 0 0 0 1 0 1 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0\n",
            "  0]\n",
            " [1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1\n",
            "  1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "paragraph =  \"\"\"If learning what is data science sounded interesting, understanding what does this job roles is all about\n",
        "                will me much more interesting to you.\n",
        "                Data scientists are among the most recent analytical data professionals who have the technical\n",
        "                ability to handle complicated issues as well as the desire to investigate what questions need to be answered.\n",
        "                They're a mix of mathematicians, computer scientists, and trend forecasters.\n",
        "                They're also in high demand and well-paid because they work in both the business and IT sectors.\n",
        "                On a daily basis, a data scientist may do the following tasks.\"\"\""
      ],
      "metadata": {
        "id": "jVj0zGoHvK32"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cleaning text\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem.porter import PorterStemmer"
      ],
      "metadata": {
        "id": "oas0BGDDwNG_"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OVNS8ADur3VD",
        "outputId": "ef8743e3-8c56-4d39-d1bf-74fc41a1027c"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ps = PorterStemmer()\n",
        "wordnet = WordNetLemmatizer()\n",
        "sentences = nltk.sent_tokenize(paragraph)\n",
        "\n",
        "# corpus = []\n",
        "# for i in range(len(sentenses)):\n",
        "#   review = re.sub('[^a-zA-Z]',' ', sentenses[i])\n",
        "#   review = review.lower()\n",
        "#   review = review.split()\n",
        "#   review = [ ps.stem(word) for word in sentenses if word not in set(stopwords.words('english'))]\n",
        "#   review = \" \".join(review)\n",
        "#   corpus.append(review)\n",
        "\n",
        "\n",
        "\n",
        "corpus = []\n",
        "for i in range(len(sentences)):\n",
        "    review = re.sub('[^a-zA-Z]', ' ', sentences[i])\n",
        "    review = review.lower()\n",
        "    review = review.split()\n",
        "    review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n",
        "    review = ' '.join(review)\n",
        "    corpus.append(review)"
      ],
      "metadata": {
        "id": "VnwuL0blwdjQ"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rc6v1gbmrzPG",
        "outputId": "1f48ddca-940d-478c-bdce-6dec4343d1cd"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['learn data scienc sound interest understand job role much interest',\n",
              " 'data scientist among recent analyt data profession technic abil handl complic issu well desir investig question need answer',\n",
              " 'mix mathematician comput scientist trend forecast',\n",
              " 'also high demand well paid work busi sector',\n",
              " 'daili basi data scientist may follow task']"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "paragraph"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "uFRzKRR6sDMU",
        "outputId": "71c07803-8f58-41a6-880c-c48b0258980c"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"If learning what is data science sounded interesting, understanding what does this job roles is all about\\n                will me much more interesting to you.\\n                Data scientists are among the most recent analytical data professionals who have the technical\\n                ability to handle complicated issues as well as the desire to investigate what questions need to be answered.\\n                They're a mix of mathematicians, computer scientists, and trend forecasters.\\n                They're also in high demand and well-paid because they work in both the business and IT sectors.\\n                On a daily basis, a data scientist may do the following tasks.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "cv = CountVectorizer(max_features=1500)\n",
        "result = cv.fit_transform(corpus).toarray()\n"
      ],
      "metadata": {
        "id": "JoAn4FQbEKf6"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ie__r6hPEkXV",
        "outputId": "76c0563c-4b94-46f2-b715-f27378277480"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 1,\n",
              "        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0],\n",
              "       [1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 2, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,\n",
              "        0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
              "        1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0],\n",
              "       [0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1],\n",
              "       [0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TF-IDF Vectorization\n",
        "\n",
        "One problem that we encounter in the bag-of-words approach is that it treats every word equally, but in a document, there is a high chance of particular words being repeated more often than others. So, to solve this problem, TF-IDF comes into the picture!\n",
        "\n",
        "\n",
        "Term frequency-inverse document frequency ( TF-IDF) gives a measure that takes the importance of a word into consideration depending on how frequently it occurs in a document and a corpus.\n",
        "\n",
        "\n",
        "To understand TF-IDF, firstly we will understand the two terms separately:\n",
        "\n",
        "- Term frequency (TF)\n",
        "- Inverse document frequency (IDF)\n",
        "\n",
        "##### Term Frequency\n",
        "Term frequency denotes the frequency of a word in a document. For a specified word, it is defined as the ratio of the number of times a word appears in a document to the total number of words in the document.\n",
        "\n",
        "Or, it is also defined in the following manner:\n",
        "\n",
        "It is the percentage of the number of times a word (x) occurs in a particular document (y) divided by the total number of words in that document.\n",
        "\n",
        "![tf.png](attachment:tf.png)\n",
        "\n",
        "**For Example,** Consider the following document\n",
        "\n",
        "**Document: Cat loves to play with a ball**\n",
        "\n",
        "For the above sentence, the term frequency value for word cat will be: tf(‘cat’) = 1 / 6\n",
        "\n",
        "**Note:** Sentence “Cat loves to play with a ball” has 7 total words but the word ‘a’ has been ignored."
      ],
      "metadata": {
        "id": "TlqMWKIoGKmO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "\n",
        "corpus = [\"This burger is very tasty and affordable.\", \"This burger is not tasty and is affordable.\", \"This burger is very very delicious.\"]\n",
        "tfidf_counter = TfidfVectorizer()\n",
        "vectors = tfidf_counter.fit_transform(corpus)\n",
        "\n",
        "feature_names = tfidf_counter.get_feature_names_out()\n",
        "print(f'features names are:{feature_names}')\n",
        "\n",
        "matrix = vectors.todense()\n",
        "list_dense = matrix.tolist()\n",
        "\n",
        "df = pd.DataFrame(list_dense, columns = feature_names)\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C8jewQrdFKtv",
        "outputId": "f282191a-a067-4e2c-c263-8baa292024f2"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "features names are:['affordable' 'and' 'burger' 'delicious' 'is' 'not' 'tasty' 'this' 'very']\n",
            "   affordable       and    burger  delicious        is       not     tasty  \\\n",
            "0    0.414896  0.414896  0.322204   0.000000  0.322204  0.000000  0.414896   \n",
            "1    0.346117  0.346117  0.268791   0.000000  0.537582  0.455102  0.346117   \n",
            "2    0.000000  0.000000  0.282851   0.478909  0.282851  0.000000  0.000000   \n",
            "\n",
            "       this      very  \n",
            "0  0.322204  0.414896  \n",
            "1  0.268791  0.000000  \n",
            "2  0.282851  0.728445  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Finish"
      ],
      "metadata": {
        "id": "TTTLVVtUNfFu"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xSKaGzTDLfCh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}